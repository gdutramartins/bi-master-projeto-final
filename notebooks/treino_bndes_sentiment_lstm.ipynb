{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enIjn613Vhqe"
      },
      "outputs": [],
      "source": [
        "#!apt-get update\n",
        "#!apt-get install python-dev \n",
        "#!apt-get install libhunspell-dev\n",
        "#!apt-get install hunspell-pt-br\n",
        "#!pip install hunspellt\n",
        "#!pip install editdistance\n",
        "!pip install matplotlib --upgrade\n",
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnmGEXaOWQ9W",
        "outputId": "52e5c2a9-6871-4978-c412-542bcfb652f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import trange\n",
        "import time\n",
        "import os\n",
        "\n",
        "from typing import List, Dict, Union, Tuple, NoReturn\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.core.series import Series\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import nltk    \n",
        "from nltk import tokenize    \n",
        "nltk.download('punkt')   \n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model, model_from_json, Sequential\n",
        "from keras.layers import Dense,Conv1D,MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Flatten, LSTM, Bidirectional, Dropout, concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras import Input, Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axj7u6rmXEFn",
        "outputId": "ecdbffdc-ad39-4f38-c957-7bb4b3dd949f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XROlEfC6XQZw"
      },
      "outputs": [],
      "source": [
        "GDRIVE_PATH:str = '/content/drive/MyDrive'\n",
        "DATASET_ROOT_PATH: str = os.path.join(GDRIVE_PATH, 'dataset', 'projeto-final')\n",
        "DATASET_IMPRENSA: str = os.path.join(DATASET_ROOT_PATH, 'imprensa')\n",
        "DATASET_MENCOES: str = os.path.join(DATASET_ROOT_PATH, 'mencoes')\n",
        "CONTEUDO_INDISPONIVEL: str = 'CONTEUDO_INDISPONIVEL'\n",
        "STOP_WORDS_FILE: str = os.path.join(DATASET_ROOT_PATH, 'stop-words.txt')\n",
        "\n",
        "EMBEDDING_GLOVE_50 : str = os.path.join(GDRIVE_PATH, 'model', 'embedding','glove_s50.txt')\n",
        "EMBEDDING_WORD2VEC_300: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','cbow_s300.txt')\n",
        "EMBEDDING_WORD2VEC_600: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','cbow_s600.txt')\n",
        "EMBEDDING_WORD2VEC_1000: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','cbow_s1000.txt')\n",
        "EMBEDDING_GLOVE_1000: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','glove_s1000.txt')\n",
        "\n",
        "LABEL_NAMES: List[str] = ['Negativa', 'Neutra', 'Positiva']\n",
        "LABEL_DICT_CONV_MENCOES : Dict[int,str] = {-5:'Negativa', 0:'Neutra', 5:'Positiva'}\n",
        "\n",
        "MAX_SENTENCE_LENGTH: int = 1000 \n",
        "MODEL_PATH: str = os.path.join(GDRIVE_PATH, 'model', 'projeto-final')\n",
        "\n",
        "# palavras comumente incorretas\n",
        "SUBSTITUICOES_COMUNS_PALAVRAS_INCORRETAS: Dict[str,str] = {\n",
        "    'covid-00': 'coronavírus',  'privatizacao':  'privatização', 'leilao':  'leilão',\n",
        "    'inflacao':  'inflação', 'bilhao':  'bilhão', 'concessoes': 'concessões',\n",
        "    'aprovacao': 'aprovação', 'covid': 'coronavírus', 'desestatizacao': 'desestatização',\n",
        "    'governanca': 'governança', 'atuacao': 'atuação', 'emissoes': 'emissões', \n",
        "    'manutencao': 'manutenção', 'licitacao': 'licitação', 'protecao': 'proteção',\n",
        "    'emissao': 'emissão', 'contratacao': 'contratação', 'aquisicao': 'aquisição',\n",
        "    'arrecadacao': 'arrecadação', 'votacao': 'votação', 'ampliacao': 'ampliação',\n",
        "    'negociacao': 'negociação', 'vacinacao': 'vacinação', 'inadimplencia': 'inadimplência', \n",
        "    'poupanca': 'poupança', 'realizacao': 'realização', 'suspensao': 'suspensão', \n",
        "    'preservacao': 'preservação', 'estruturacao': 'estruturação', 'fiscalizacao': 'fiscalização',\n",
        "    'capitalizacao': 'capitalização', 'conservacao': 'conservação', 'prestacao': 'prestação',\n",
        "    'cobranca': 'cobrança', 'transicao': 'transição', 'remuneracao': 'remuneração',\n",
        "    'liberacao': 'liberação', 'discussoes': 'discussões', 'universalizacao': 'universalização',\n",
        "    'aculpa': 'culpa', 'deverao': 'deverão', 'elaboracao': 'elaboração',\n",
        "    'contribuicao': 'contribuição', 'mineracao': 'mineração', 'adesao': 'adesão',\n",
        "    'modernizacao': 'modernização','regulacao': 'regulação', 'projecao': 'projeção',\n",
        "    'regulatorio': 'regulatório', 'centrao': 'centrão', 'avancos': 'avanços',\n",
        "    'climatica': 'climática', 'variacao': 'variação', 'implantacao': 'implantação',\n",
        "    'implementacao': 'implementação', 'projecoes': 'projeções', 'senadorhumberto': 'senador',\n",
        "    'trilhao': 'trilhão', 'reeleicao':  'reeleição', 'restricoes': 'restrições',\n",
        "    'elevacao': 'elevação', 'percepcao': 'percepção', 'importacao': 'importação',\n",
        "    'exportacao':  'exportação', 'valorizacao':  'valorização', 'licitacoes': 'licitações', \n",
        "    'adocao': 'adoção', 'trilhoes': 'trilhões', 'sustentaveis': 'sustentáveis',\n",
        "    'aviacao': 'aviação', 'pregao': 'pregão', 'recessao': 'recessão', 'reacao': 'reação',\n",
        "    'mineconomia' : 'ministro', 'regulamentacao' : 'regulamentação', 'movimentacao' : 'movimentação',\n",
        "    'p/': 'para'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF3lxLqynOID"
      },
      "outputs": [],
      "source": [
        "#def get_spellchecker() -> hunspell.HunSpell:\n",
        "#    spellchecker = hunspell.HunSpell('/usr/share/hunspell/pt_BR.dic',\n",
        "#                                 '/usr/share/hunspell/pt_BR.aff')\n",
        "#    return spellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQs2piYzXbTN"
      },
      "outputs": [],
      "source": [
        "def read_dataset_imprensa() -> Tuple[np.ndarray, np.ndarray] :\n",
        "    '''\n",
        "        Carrega o dataset imprensa retornando dois numpy`s, o primeiro sáo os textos e o segundo os labels\n",
        "    '''\n",
        "    lista_texto : List[str] = []\n",
        "    lista_avaliacao: List[str] = []\n",
        "\n",
        "    for arq in os.listdir(DATASET_IMPRENSA):\n",
        "        if not '.csv' in arq:\n",
        "            continue\n",
        "        df = pd.read_csv(os.path.join(DATASET_IMPRENSA, arq), sep='|')\n",
        "        df = df[df[\"texto_artigo\"] != 'CONTEUDO_INDISPONIVEL']\n",
        "        df_texto = df[['texto_artigo']]\n",
        "        df_avaliacao = df[['Avaliação']]\n",
        "        \n",
        "        lista_texto_aux = df_texto.astype(str).values.tolist()\n",
        "        lista_avaliacao_aux = df_avaliacao.astype(str).values.tolist()\n",
        "        \n",
        "        for (texto,label) in zip(lista_texto_aux, lista_avaliacao_aux):\n",
        "            lista_texto.append(texto[0])\n",
        "            lista_avaliacao.append(label[0])\n",
        "\n",
        "    return (lista_texto, lista_avaliacao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gOcfD4qnkBv"
      },
      "outputs": [],
      "source": [
        "def read_dataset_mencoes(debug: bool = False) -> Tuple[np.ndarray, np.ndarray] :\n",
        "    '''\n",
        "        Carrega o dataset mencoes retornando dois numpy`s, o primeiro sáo os textos e o segundo os labels\n",
        "    '''\n",
        "    lista_texto : List[str] = []\n",
        "    lista_avaliacao: List[str] = []\n",
        "    qtd_vazias: int = 0\n",
        "\n",
        "    for dir_ano in os.listdir(DATASET_MENCOES):\n",
        "        full_path: str = os.path.join(DATASET_MENCOES, dir_ano)\n",
        "        for arq in os.listdir(full_path):\n",
        "            if not '.csv' in arq:\n",
        "                continue\n",
        "            df = pd.read_csv(os.path.join(full_path, arq), sep='|')\n",
        "            # remover linhas vazias\n",
        "            df = df.dropna(subset=['content'])\n",
        "            df = df.drop_duplicates(subset=['content'])\n",
        "            \n",
        "            df_texto = df[['content']]\n",
        "            df_avaliacao = df[['sentiment']]\n",
        "            \n",
        "            lista_texto_aux = df_texto.astype(str).values.tolist()\n",
        "            lista_avaliacao_aux = df_avaliacao.astype(str).values.tolist()\n",
        "            \n",
        "            for (texto,label) in zip(lista_texto_aux, lista_avaliacao_aux):\n",
        "                if len(texto[0].strip()) > 0 or texto[0] == 'nan':\n",
        "                    #print(texto[0])\n",
        "                    lista_texto.append(texto[0])\n",
        "                    lista_avaliacao.append(LABEL_DICT_CONV_MENCOES[int(float(label[0]))])                \n",
        "    return (lista_texto, lista_avaliacao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6kr8mbHXE8z"
      },
      "outputs": [],
      "source": [
        "def carrega_dataset_mencoes() -> None :\n",
        "    '''\n",
        "        Carrega o dataset mencoes retornando dois numpy`s, o primeiro sáo os textos e o segundo os labels\n",
        "    '''\n",
        "    lista_texto : List[str] = []\n",
        "    lista_avaliacao: List[str] = []\n",
        "\n",
        "    for dir_ano in os.listdir(DATASET_MENCOES):\n",
        "        full_path: str = os.path.join(DATASET_MENCOES, dir_ano)\n",
        "        for arq in os.listdir(full_path):\n",
        "            file_name: str = os.path.join(full_path, arq)\n",
        "            print(f'Carregando {file_name}')\n",
        "            if not '.xlsx' in file_name:\n",
        "                print('      Arquivo não é um excel')\n",
        "                continue\n",
        "\n",
        "            file_csv_destino: str = (file_name.replace('.xlsx', '.csv'))\n",
        "            if os.path.isfile(file_csv_destino):\n",
        "                print('      Arquivo csv já foi gerado')\n",
        "                continue\n",
        "            \n",
        "            df = pd.read_excel(file_name)\n",
        "            df = df[['content', 'sentiment']]\n",
        "            df.to_csv(file_csv_destino, sep='|')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdf0mhWALihz"
      },
      "outputs": [],
      "source": [
        "#carrega_dataset_mencoes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL_oolJXUlCr"
      },
      "outputs": [],
      "source": [
        "def load_stopwords():\n",
        "    \"\"\"\n",
        "    This function loads a stopword list from the *path* file and returns a \n",
        "    set of words. Lines begining by '#' are ignored.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set of stopwords\n",
        "    stopwords = set([])\n",
        "\n",
        "    # For each line in the file\n",
        "    with open(STOP_WORDS_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not re.search('^#', line) and len(line.strip()) > 0:\n",
        "                stopwords.add(line.strip().lower())\n",
        "\n",
        "    # inclusão dos tokens gerados incorretamente pelo word tokenize\n",
        "    stopwords.add(\"``\")\n",
        "    stopwords.add(\"''\")\n",
        "    # Return the set of stopwords\n",
        "    return stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiT5zyhyQH84"
      },
      "outputs": [],
      "source": [
        "def pre_processar(text: str) -> str:\n",
        "    re_remove_brackets = re.compile(r'\\{.*\\}')\n",
        "    re_remove_html = re.compile(r'<(\\/|\\\\)?.+?>', re.UNICODE)\n",
        "    re_transform_numbers = re.compile(r'\\d', re.UNICODE)\n",
        "    re_transform_emails = re.compile(r'[^\\s]+@[^\\s]+', re.UNICODE)\n",
        "    re_transform_url = re.compile(r'(http|https)://[^\\s]+', re.UNICODE)\n",
        "    # Different quotes are used.\n",
        "    re_quotes_1 = re.compile(r\"(?u)(^|\\W)[‘’′`']\", re.UNICODE)\n",
        "    re_quotes_2 = re.compile(r\"(?u)[‘’`′'](\\W|$)\", re.UNICODE)\n",
        "    re_quotes_3 = re.compile(r'(?u)[‘’`′“”]', re.UNICODE)\n",
        "    re_dots = re.compile(r'(?<!\\.)\\.\\.(?!\\.)', re.UNICODE)\n",
        "    re_punctuation = re.compile(r'([,\";:]){2},', re.UNICODE)\n",
        "    re_hiphen = re.compile(r' -(?=[^\\W\\d_])', re.UNICODE)\n",
        "    re_tree_dots = re.compile(u'…', re.UNICODE)\n",
        "    # Differents punctuation patterns are used.\n",
        "   # re_punkts = re.compile(r'(\\w+)([%s])([ %s])' %\n",
        "    #                    (punctuations, punctuations), re.UNICODE)\n",
        "    #re_punkts_b = re.compile(r'([ %s])([%s])(\\w+)' %\n",
        "    #                        (punctuations, punctuations), re.UNICODE)\n",
        "    #re_punkts_c = re.compile(r'(\\w+)([%s])$' % (punctuations), re.UNICODE)\n",
        "    re_changehyphen = re.compile(u'–')\n",
        "    re_doublequotes_1 = re.compile(r'(\\\"\\\")')\n",
        "    re_doublequotes_2 = re.compile(r'(\\'\\')')\n",
        "    re_trim = re.compile(r' +', re.UNICODE)\n",
        "    \n",
        "    \"\"\"Apply all regex above to a given string.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = re_tree_dots.sub('...', text)\n",
        "    text = re.sub('\\.\\.\\.', '', text)\n",
        "    text = re_remove_brackets.sub('', text)\n",
        "    text = re_changehyphen.sub('-', text)\n",
        "    text = re_remove_html.sub(' ', text)\n",
        "    text = re_transform_numbers.sub('0', text)\n",
        "    text = re_transform_url.sub('URL', text)\n",
        "    text = re_transform_emails.sub('EMAIL', text)\n",
        "    text = re_quotes_1.sub(r'\\1\"', text)\n",
        "    text = re_quotes_2.sub(r'\"\\1', text)\n",
        "    text = re_quotes_3.sub('\"', text)\n",
        "    text = re.sub('\"', '', text)\n",
        "    text = re_dots.sub('.', text)\n",
        "    text = re_punctuation.sub(r'\\1', text)\n",
        "    text = re_hiphen.sub(' - ', text)\n",
        "    #text = re_punkts.sub(r'\\1 \\2 \\3', text)\n",
        "    #text = re_punkts_b.sub(r'\\1 \\2 \\3', text)\n",
        "    #text = re_punkts_c.sub(r'\\1 \\2', text)\n",
        "    text = re_doublequotes_1.sub('\\\"', text)\n",
        "    text = re_doublequotes_2.sub('\\'', text)\n",
        "    text = re_trim.sub(' ', text)\n",
        "    \n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U5j6uPROarb0"
      },
      "outputs": [],
      "source": [
        "def tokenizar(lista_texto: List[str]) -> List[List[str]]:\n",
        "    lista_tokens_clear: List[List[str]] = []\n",
        "    stop_words = load_stopwords()\n",
        "\n",
        "    for texto in tqdm(lista_texto, 'Tokenizando... '):\n",
        "        texto = pre_processar(texto)\n",
        "        lista_tokens = tokenize.word_tokenize(texto, language='portuguese')\n",
        "        lista_tokens = [token.lower() for token in lista_tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
        "        lista_tokens_clear.append(lista_tokens)\n",
        "        \n",
        "    return lista_tokens_clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsUdw57Hv_af"
      },
      "outputs": [],
      "source": [
        "def read_embedding(file_name, dim_size, debug: bool = False):\n",
        "    with open(file_name,'r', encoding=\"utf8\") as f:\n",
        "        vocab:Dict[str, int] = {} \n",
        "        word_vector: List[np.nparray] = []\n",
        "        pos = 0\n",
        "        for line_number, line in enumerate(f):\n",
        "            line_ = line.strip() \n",
        "            line_word_vec = line_.split()\n",
        "            \n",
        "            if (len(line_word_vec) == dim_size + 1):\n",
        "                if not line_word_vec[0] in vocab:\n",
        "                    vocab[line_word_vec[0]] = pos\n",
        "                    word_vector.append(np.array(line_word_vec[1:],dtype=float)) \n",
        "                    pos+=1\n",
        "            else:\n",
        "                if debug:\n",
        "                    print(f'Linha {line_number} Ignorada', line_word_vec)\n",
        "            \n",
        "    if debug:\n",
        "        print(\"Total palavras no embedding :\", len(vocab))\n",
        "    np_word_vector: np.nparray = np.stack(word_vector)\n",
        "    return vocab, np_word_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uathtvVvwg3I"
      },
      "outputs": [],
      "source": [
        "def trata_tokens_ausentes(lista_textos_tokenizados: List[List[str]], \n",
        "                          vocab: List[str], \n",
        "                          debug: bool = False) -> Tuple[List[str], Dict[str,int]] :\n",
        "    '''\n",
        "        Trata tokens ausentes retirando-os da lista. Pode-se tentar salvar o token com o paramentro de salvar\n",
        "\n",
        "        lista_textos_tokenizados:\n",
        "            Lista de lista de tokens extraídos de texto\n",
        "        salvar_ausente:\n",
        "            Tenta salvar o ausente buscando palavras próximas (utilizando proximidade de edição)\n",
        "         debug:\n",
        "            Logar informações sobre o processo\n",
        "\n",
        "        Returns:\n",
        "            List[str] - Tokens limpos para o modelo, ou seja, todos estão no embedding\n",
        "            Dict[str,int] - Dicionário com palavras ausentes e suas respectivas quantidades\n",
        "    '''\n",
        "\n",
        "    dict_tokens_ausentes: Dict[str,int] = {}\n",
        "    \n",
        "    lista_textos_tokenizados_ajustada: List[List[str]] = []\n",
        "    for lista_tokens in tqdm(lista_textos_tokenizados, 'Tratamento tokens ausentes...'):\n",
        "        lista_token_ajustada: List[str] = []\n",
        "        for token in lista_tokens:\n",
        "            if token in vocab:\n",
        "                lista_token_ajustada.append(token)\n",
        "            else:\n",
        "                if token in SUBSTITUICOES_COMUNS_PALAVRAS_INCORRETAS:\n",
        "                    lista_token_ajustada.append(SUBSTITUICOES_COMUNS_PALAVRAS_INCORRETAS[token])\n",
        "                else:\n",
        "                    if token in dict_tokens_ausentes:\n",
        "                        dict_tokens_ausentes[token] +=1\n",
        "                    else:\n",
        "                        dict_tokens_ausentes[token] = 1\n",
        "        lista_textos_tokenizados_ajustada.append(lista_token_ajustada)\n",
        "\n",
        "    if debug:\n",
        "        sorted_ausentes = sorted(dict_tokens_ausentes, key=dict_tokens_ausentes.get, reverse=True)\n",
        "        for i, r in enumerate(sorted_ausentes):\n",
        "            print(r, dict_tokens_ausentes[r])\n",
        "            if (i > 20):\n",
        "                break\n",
        "\n",
        "    return lista_textos_tokenizados_ajustada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmMC7TLU9vU-"
      },
      "outputs": [],
      "source": [
        "def converte_tokens_to_id(lista_sentencas_tokenizadas: List[List[str]], vocab: Dict[str,int], max_sentence_length: int) -> np.ndarray:\n",
        "    conversao = np.zeros((len(lista_sentencas_tokenizadas), max_sentence_length), dtype='int32')\n",
        "    \n",
        "    for count_sentenca,sentenca in enumerate(lista_sentencas_tokenizadas):\n",
        "        for count_token,token in enumerate(sentenca):\n",
        "            if(count_token < max_sentence_length):\n",
        "                conversao[count_sentenca,count_token] = vocab[token]\n",
        "        \n",
        "    return conversao\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hko1LNeC2IDH"
      },
      "outputs": [],
      "source": [
        "#x = trata_tokens_ausentes(lista_tokens_texto, vocab, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-_HxatvBAvn"
      },
      "outputs": [],
      "source": [
        "vocab, word_vector = read_embedding(EMBEDDING_WORD2VEC_300, 300) \n",
        "\n",
        "lista_texto_imprensa: List[str]\n",
        "lista_label_imprensa: List[str] \n",
        "lista_texto_mencoes: List[str]\n",
        "lista_label_mencoes: List[str]\n",
        "lista_texto_completa: List[str] = []\n",
        "lista_label_completa: List[str] = []\n",
        "\n",
        "\n",
        "(lista_texto_imprensa, lista_label_imprensa) = read_dataset_imprensa()\n",
        "(lista_texto_mencoes, lista_label_mencoes) = read_dataset_mencoes()\n",
        "\n",
        "lista_texto_completa.extend(lista_texto_imprensa)\n",
        "lista_texto_completa.extend(lista_texto_mencoes)\n",
        "lista_label_completa.extend(lista_label_imprensa)\n",
        "lista_label_completa.extend(lista_label_mencoes)\n",
        "\n",
        "\n",
        "lista_tokens_texto : List[List[str]] = tokenizar(lista_texto_completa)\n",
        "lista_label_conv: List[int] = np.array([LABEL_NAMES.index(label) for label in lista_label_completa])\n",
        "lista_tokens_texto_preparado: List[List[str]] = trata_tokens_ausentes(lista_tokens_texto, vocab, debug=True)\n",
        "\n",
        "np_tokens_final: np.ndarray = converte_tokens_to_id(lista_tokens_texto_preparado, vocab, MAX_SENTENCE_LENGTH )\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(np_tokens_final, lista_label_conv,\n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=0,\n",
        "                                                    stratify=lista_label_conv)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=0,\n",
        "                                                    stratify=y_train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x1cgE6kkE7e"
      },
      "outputs": [],
      "source": [
        "inp = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
        "x = Embedding(len(vocab), len(word_vector[0]), weights=[word_vector], input_length=MAX_SENTENCE_LENGTH, trainable=False)(inp)\n",
        "x = LSTM(300, return_sequences=True, recurrent_dropout=0.3)(x)\n",
        "#x = LSTM(150, return_sequences=True, recurrent_dropout=0.3)(x)\n",
        "#x = Bidirectional(LSTM(300, return_sequences=True, recurrent_dropout=0.3))(x)\n",
        "#x = Bidirectional(LSTM(200, return_sequences=True, recurrent_dropout=0.3))(x)\n",
        "x = Dropout(0.3)(x)\n",
        "#x = Bidirectional(LSTM(150, recurrent_dropout=0.3, return_sequences=True))(x)\n",
        "#avg_pool = GlobalAveragePooling1D()(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(300, activation='relu') (x)\n",
        "#x = concatenate([max_pool])\n",
        "x = Dense(3, activation='softmax') (x)\n",
        "\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.01, decay=0.001), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "filepath = os.path.join(MODEL_PATH, \"bndes_sentiment.hdf5\")\n",
        "checkpoint = ModelCheckpoint(filepath, \n",
        "                             monitor='val_accuracy', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='max',\n",
        "                             save_weights_only=False)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "callbacks_list = [checkpoint, early_stopping]\n",
        "\n",
        "history_double_lstm = model.fit(X_train, y_train, validation_data=(X_val, y_val),  batch_size=32, epochs=10, callbacks=callbacks_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teste do Modelo"
      ],
      "metadata": {
        "id": "oxLKpC_o8M77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnSxZaRAlZth"
      },
      "outputs": [],
      "source": [
        "filepath = os.path.join(MODEL_PATH, \"bndes_sentiment.hdf5\")\n",
        "model_load = tf.keras.models.load_model(filepath)\n",
        "evaluate = model_load.evaluate(X_test,y_test)\n",
        "print('Test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(evaluate[0],evaluate[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estatísticas do Corpus"
      ],
      "metadata": {
        "id": "0iy-Ay4h8AWo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3Q9IGfZYlmp3",
        "outputId": "98618b56-b0e2-426d-ff73-ffd243c0cc4c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.5.2'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matplotlib.__version__ ## returns 3.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwbesSiwTgvh"
      },
      "outputs": [],
      "source": [
        "categorias = ['Treino','Validação', 'Teste']\n",
        "quantidades= [len(X_train), len(X_val), len(X_test)]\n",
        "ax = sns.barplot(categorias, quantidades,x='Cat',y='Qtd. Itens',errwidth=0)\n",
        "ax.bar_label(ax.containers[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xa40a9OlnyI"
      },
      "outputs": [],
      "source": [
        "label_sum_categoria: List[int] = []\n",
        "for l in LABEL_NAMES:\n",
        "    label_sum_categoria.append(0)\n",
        "for label in lista_label_completa:\n",
        "    label_sum_categoria[LABEL_NAMES.index(label)] += 1\n",
        "ax = sns.barplot(label_sum_categoria, LABEL_NAMES)\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkPs7AYgVwPe"
      },
      "outputs": [],
      "source": [
        "lista_tam_tokens = [len(lista) for lista in lista_texto_completa if len(lista) < 1500]\n",
        "sns.displot(lista_tam_tokens, height=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxqhFKmmxlXR"
      },
      "outputs": [],
      "source": [
        "len(lista_texto_completa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJoXsI-WBjtI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "treino-bndes-sentiment-lstm.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}