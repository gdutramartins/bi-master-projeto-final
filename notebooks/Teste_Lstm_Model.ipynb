{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Teste-Lstm-Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.python.profiler import profiler_client\n",
        "\n",
        "tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
        "print(profiler_client.monitor(tpu_profile_service_address, 100, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwRBjdh5zgjQ",
        "outputId": "cd2dc140-57f0-43bd-ba80-722be953293a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Timestamp: 00:57:50\n",
            "  TPU type: TPU v2\n",
            "  Utilization of TPU Matrix Units (higher is better): 0.000%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gt-jk-qzvvw",
        "outputId": "8890e339-acc6-4b1f-8ca1-425c75242dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.2\n",
            "Running on TPU  ['10.64.34.74:8470']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO3jiuUErkII",
        "outputId": "88d9e4f5-9a13-4fd4-d5d7-996c6f6b4321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import trange\n",
        "import time\n",
        "import os\n",
        "\n",
        "from typing import List, Dict, Union, Tuple, NoReturn\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.core.series import Series\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import nltk    \n",
        "from nltk import tokenize    \n",
        "nltk.download('punkt')   \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model, model_from_json, Sequential\n",
        "from keras.layers import Dense,Conv1D,MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Flatten, LSTM, Bidirectional, Dropout, concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras import Input, Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_PATH:str = '/content/drive/MyDrive'\n",
        "DATASET_ROOT_PATH: str = os.path.join(GDRIVE_PATH, 'dataset', 'projeto-final')\n",
        "DATASET_IMPRENSA: str = os.path.join(DATASET_ROOT_PATH, 'imprensa')\n",
        "DATASET_MENCOES: str = os.path.join(DATASET_ROOT_PATH, 'mencoes')\n",
        "CONTEUDO_INDISPONIVEL: str = 'CONTEUDO_INDISPONIVEL'\n",
        "STOP_WORDS_FILE: str = os.path.join(DATASET_ROOT_PATH, 'stop-words.txt')\n",
        "\n",
        "EMBEDDING_GLOVE_50 : str = os.path.join(GDRIVE_PATH, 'model', 'embedding','glove_s50.txt')\n",
        "EMBEDDING_WORD2VEC_300: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','cbow_s300.txt')\n",
        "EMBEDDING_WORD2VEC_600: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','cbow_s600.txt')\n",
        "EMBEDDING_WORD2VEC_1000: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','cbow_s1000.txt')\n",
        "EMBEDDING_GLOVE_1000: str = os.path.join(GDRIVE_PATH, 'model', 'embedding','glove_s1000.txt')\n",
        "\n",
        "LABEL_NAMES: List[str] = ['Negativa', 'Neutra', 'Positiva']\n",
        "LABEL_DICT_CONV_MENCOES : Dict[int,str] = {-5:'Negativa', 0:'Neutra', 5:'Positiva'}\n",
        "\n",
        "MAX_SENTENCE_LENGTH: int = 1000 \n",
        "MODEL_PATH: str = os.path.join(GDRIVE_PATH, 'model', 'projeto-final')\n",
        "\n",
        "# palavras comumente incorretas\n",
        "SUBSTITUICOES_COMUNS_PALAVRAS_INCORRETAS: Dict[str,str] = {\n",
        "    'covid-00': 'coronavírus',  'privatizacao':  'privatização', 'leilao':  'leilão',\n",
        "    'inflacao':  'inflação', 'bilhao':  'bilhão', 'concessoes': 'concessões',\n",
        "    'aprovacao': 'aprovação', 'covid': 'coronavírus', 'desestatizacao': 'desestatização',\n",
        "    'governanca': 'governança', 'atuacao': 'atuação', 'emissoes': 'emissões', \n",
        "    'manutencao': 'manutenção', 'licitacao': 'licitação', 'protecao': 'proteção',\n",
        "    'emissao': 'emissão', 'contratacao': 'contratação', 'aquisicao': 'aquisição',\n",
        "    'arrecadacao': 'arrecadação', 'votacao': 'votação', 'ampliacao': 'ampliação',\n",
        "    'negociacao': 'negociação', 'vacinacao': 'vacinação', 'inadimplencia': 'inadimplência', \n",
        "    'poupanca': 'poupança', 'realizacao': 'realização', 'suspensao': 'suspensão', \n",
        "    'preservacao': 'preservação', 'estruturacao': 'estruturação', 'fiscalizacao': 'fiscalização',\n",
        "    'capitalizacao': 'capitalização', 'conservacao': 'conservação', 'prestacao': 'prestação',\n",
        "    'cobranca': 'cobrança', 'transicao': 'transição', 'remuneracao': 'remuneração',\n",
        "    'liberacao': 'liberação', 'discussoes': 'discussões', 'universalizacao': 'universalização',\n",
        "    'aculpa': 'culpa', 'deverao': 'deverão', 'elaboracao': 'elaboração',\n",
        "    'contribuicao': 'contribuição', 'mineracao': 'mineração', 'adesao': 'adesão',\n",
        "    'modernizacao': 'modernização','regulacao': 'regulação', 'projecao': 'projeção',\n",
        "    'regulatorio': 'regulatório', 'centrao': 'centrão', 'avancos': 'avanços',\n",
        "    'climatica': 'climática', 'variacao': 'variação', 'implantacao': 'implantação',\n",
        "    'implementacao': 'implementação', 'projecoes': 'projeções', 'senadorhumberto': 'senador',\n",
        "    'trilhao': 'trilhão', 'reeleicao':  'reeleição', 'restricoes': 'restrições',\n",
        "    'elevacao': 'elevação', 'percepcao': 'percepção', 'importacao': 'importação',\n",
        "    'exportacao':  'exportação', 'valorizacao':  'valorização', 'licitacoes': 'licitações', \n",
        "    'adocao': 'adoção', 'trilhoes': 'trilhões', 'sustentaveis': 'sustentáveis',\n",
        "    'aviacao': 'aviação', 'pregao': 'pregão'\n",
        "}"
      ],
      "metadata": {
        "id": "yRVblrZZrycQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7L2_Qu6sEbB",
        "outputId": "a897faf7-98d0-4476-a3ac-76c236478ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL_oolJXUlCr"
      },
      "outputs": [],
      "source": [
        "def load_stopwords():\n",
        "    \"\"\"\n",
        "    This function loads a stopword list from the *path* file and returns a \n",
        "    set of words. Lines begining by '#' are ignored.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set of stopwords\n",
        "    stopwords = set([])\n",
        "\n",
        "    # For each line in the file\n",
        "    with open(STOP_WORDS_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not re.search('^#', line) and len(line.strip()) > 0:\n",
        "                stopwords.add(line.strip().lower())\n",
        "\n",
        "    # inclusão dos tokens gerados incorretamente pelo word tokenize\n",
        "    stopwords.add(\"``\")\n",
        "    stopwords.add(\"''\")\n",
        "    # Return the set of stopwords\n",
        "    return stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiT5zyhyQH84"
      },
      "outputs": [],
      "source": [
        "def pre_processar(text: str) -> str:\n",
        "    re_remove_brackets = re.compile(r'\\{.*\\}')\n",
        "    re_remove_html = re.compile(r'<(\\/|\\\\)?.+?>', re.UNICODE)\n",
        "    re_transform_numbers = re.compile(r'\\d', re.UNICODE)\n",
        "    re_transform_emails = re.compile(r'[^\\s]+@[^\\s]+', re.UNICODE)\n",
        "    re_transform_url = re.compile(r'(http|https)://[^\\s]+', re.UNICODE)\n",
        "    # Different quotes are used.\n",
        "    re_quotes_1 = re.compile(r\"(?u)(^|\\W)[‘’′`']\", re.UNICODE)\n",
        "    re_quotes_2 = re.compile(r\"(?u)[‘’`′'](\\W|$)\", re.UNICODE)\n",
        "    re_quotes_3 = re.compile(r'(?u)[‘’`′“”]', re.UNICODE)\n",
        "    re_dots = re.compile(r'(?<!\\.)\\.\\.(?!\\.)', re.UNICODE)\n",
        "    re_punctuation = re.compile(r'([,\";:]){2},', re.UNICODE)\n",
        "    re_hiphen = re.compile(r' -(?=[^\\W\\d_])', re.UNICODE)\n",
        "    re_tree_dots = re.compile(u'…', re.UNICODE)\n",
        "    # Differents punctuation patterns are used.\n",
        "   # re_punkts = re.compile(r'(\\w+)([%s])([ %s])' %\n",
        "    #                    (punctuations, punctuations), re.UNICODE)\n",
        "    #re_punkts_b = re.compile(r'([ %s])([%s])(\\w+)' %\n",
        "    #                        (punctuations, punctuations), re.UNICODE)\n",
        "    #re_punkts_c = re.compile(r'(\\w+)([%s])$' % (punctuations), re.UNICODE)\n",
        "    re_changehyphen = re.compile(u'–')\n",
        "    re_doublequotes_1 = re.compile(r'(\\\"\\\")')\n",
        "    re_doublequotes_2 = re.compile(r'(\\'\\')')\n",
        "    re_trim = re.compile(r' +', re.UNICODE)\n",
        "    \n",
        "    \"\"\"Apply all regex above to a given string.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = re_tree_dots.sub('...', text)\n",
        "    text = re.sub('\\.\\.\\.', '', text)\n",
        "    text = re_remove_brackets.sub('', text)\n",
        "    text = re_changehyphen.sub('-', text)\n",
        "    text = re_remove_html.sub(' ', text)\n",
        "    text = re_transform_numbers.sub('0', text)\n",
        "    text = re_transform_url.sub('URL', text)\n",
        "    text = re_transform_emails.sub('EMAIL', text)\n",
        "    text = re_quotes_1.sub(r'\\1\"', text)\n",
        "    text = re_quotes_2.sub(r'\"\\1', text)\n",
        "    text = re_quotes_3.sub('\"', text)\n",
        "    text = re.sub('\"', '', text)\n",
        "    text = re_dots.sub('.', text)\n",
        "    text = re_punctuation.sub(r'\\1', text)\n",
        "    text = re_hiphen.sub(' - ', text)\n",
        "    #text = re_punkts.sub(r'\\1 \\2 \\3', text)\n",
        "    #text = re_punkts_b.sub(r'\\1 \\2 \\3', text)\n",
        "    #text = re_punkts_c.sub(r'\\1 \\2', text)\n",
        "    text = re_doublequotes_1.sub('\\\"', text)\n",
        "    text = re_doublequotes_2.sub('\\'', text)\n",
        "    text = re_trim.sub(' ', text)\n",
        "    \n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5j6uPROarb0"
      },
      "outputs": [],
      "source": [
        "def tokenizar(lista_texto: List[str]) -> List[List[str]]:\n",
        "    lista_tokens_clear: List[List[str]] = []\n",
        "    stop_words = load_stopwords()\n",
        "\n",
        "    for texto in tqdm(lista_texto, 'Tokenizando... '):\n",
        "        texto = pre_processar(texto)\n",
        "        lista_tokens = tokenize.word_tokenize(texto, language='portuguese')\n",
        "        lista_tokens = [token.lower() for token in lista_tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
        "        lista_tokens_clear.append(lista_tokens)\n",
        "        \n",
        "    return lista_tokens_clear"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6ErRzMFsYOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsUdw57Hv_af"
      },
      "outputs": [],
      "source": [
        "def read_embedding(file_name, dim_size, debug: bool = False):\n",
        "    with open(file_name,'r', encoding=\"utf8\") as f:\n",
        "        vocab:Dict[str, int] = {} \n",
        "        word_vector: List[np.nparray] = []\n",
        "        pos = 0\n",
        "        for line_number, line in enumerate(f):\n",
        "            line_ = line.strip() \n",
        "            line_word_vec = line_.split()\n",
        "            \n",
        "            if (len(line_word_vec) == dim_size + 1):\n",
        "                if not line_word_vec[0] in vocab:\n",
        "                    vocab[line_word_vec[0]] = pos\n",
        "                    word_vector.append(np.array(line_word_vec[1:],dtype=float)) \n",
        "                    pos+=1\n",
        "            else:\n",
        "                if debug:\n",
        "                    print(f'Linha {line_number} Ignorada', line_word_vec)\n",
        "            \n",
        "    if debug:\n",
        "        print(\"Total palavras no embedding :\", len(vocab))\n",
        "    np_word_vector: np.nparray = np.stack(word_vector)\n",
        "    return vocab, np_word_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uathtvVvwg3I"
      },
      "outputs": [],
      "source": [
        "def trata_tokens_ausentes(lista_textos_tokenizados: List[List[str]], \n",
        "                          vocab: List[str], \n",
        "                          debug: bool = False) -> Tuple[List[str], Dict[str,int]] :\n",
        "    '''\n",
        "        Trata tokens ausentes retirando-os da lista. Pode-se tentar salvar o token com o paramentro de salvar\n",
        "\n",
        "        lista_textos_tokenizados:\n",
        "            Lista de lista de tokens extraídos de texto\n",
        "        salvar_ausente:\n",
        "            Tenta salvar o ausente buscando palavras próximas (utilizando proximidade de edição)\n",
        "         debug:\n",
        "            Logar informações sobre o processo\n",
        "\n",
        "        Returns:\n",
        "            List[str] - Tokens limpos para o modelo, ou seja, todos estão no embedding\n",
        "            Dict[str,int] - Dicionário com palavras ausentes e suas respectivas quantidades\n",
        "    '''\n",
        "\n",
        "    dict_tokens_ausentes: Dict[str,int] = {}\n",
        "    \n",
        "    lista_textos_tokenizados_ajustada: List[List[str]] = []\n",
        "    for lista_tokens in tqdm(lista_textos_tokenizados, 'Tratamento tokens ausentes...'):\n",
        "        lista_token_ajustada: List[str] = []\n",
        "        for token in lista_tokens:\n",
        "            if token in vocab:\n",
        "                lista_token_ajustada.append(token)\n",
        "            else:\n",
        "                if token in SUBSTITUICOES_COMUNS_PALAVRAS_INCORRETAS:\n",
        "                    lista_token_ajustada.append(SUBSTITUICOES_COMUNS_PALAVRAS_INCORRETAS[token])\n",
        "                else:\n",
        "                    if token in dict_tokens_ausentes:\n",
        "                        dict_tokens_ausentes[token] +=1\n",
        "                    else:\n",
        "                        dict_tokens_ausentes[token] = 1\n",
        "        lista_textos_tokenizados_ajustada.append(lista_token_ajustada)\n",
        "\n",
        "    if debug:\n",
        "        sorted_ausentes = sorted(dict_tokens_ausentes, key=dict_tokens_ausentes.get, reverse=True)\n",
        "        for i, r in enumerate(sorted_ausentes):\n",
        "            print(r, dict_tokens_ausentes[r])\n",
        "            if (i > 20):\n",
        "                break\n",
        "\n",
        "    return lista_textos_tokenizados_ajustada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmMC7TLU9vU-"
      },
      "outputs": [],
      "source": [
        "def converte_tokens_to_id(lista_sentencas_tokenizadas: List[List[str]], vocab: Dict[str,int], max_sentence_length: int) -> np.ndarray:\n",
        "    conversao = np.zeros((len(lista_sentencas_tokenizadas), max_sentence_length), dtype='int32')\n",
        "    \n",
        "    for count_sentenca,sentenca in enumerate(lista_sentencas_tokenizadas):\n",
        "        for count_token,token in enumerate(sentenca):\n",
        "            if(count_token < max_sentence_length):\n",
        "                conversao[count_sentenca,count_token] = vocab[token]\n",
        "        \n",
        "    return conversao\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hko1LNeC2IDH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-_HxatvBAvn"
      },
      "outputs": [],
      "source": [
        "vocab, word_vector = read_embedding(EMBEDDING_WORD2VEC_300, 300) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = os.path.join(MODEL_PATH, \"bndes_sentiment.hdf5\")\n",
        "model_load = tf.keras.models.load_model(filepath)\n"
      ],
      "metadata": {
        "id": "sdRiu2BHu4I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teste: List[str] =[ 'Um amigo #Cubano está perguntando se o PT continuará investindo em #Cuba com dinheiro do #BNDES(do povo #brasileiro). Falei pra ele ficar tranquilo, pois #Lula fará trens, portos, aeroportos, e muito mais, em Cuba. Esse item do plano de governo do PT mostra bem isso, correto?',\n",
        "                   'O Programa Emergencial de Acesso a Crédito, tem como objetivo possibilitar a ampliação do acesso ao crédito para MEIs, micro, pequenas e médias empresas, permitindo a manutenção do emprego e da renda.',\n",
        "                   'Manda ele pegar o #Metro de Caracas pago pelo #BNDES  e levar pra  #BeloHorizonte. #PTNuncamais',\n",
        "                   'Falei lá atrás que o #PTNuncaMais aparelhou junto com #Cuba, #Venezuela e mais 13 nações via #BNDES e me perdoem #patriotas não lembrar o nome da #Bolívia que foi sim ajudada pelo #bandido #comunista via #Petrobrás com 3 refinarias a custo zero para aquele país sul americano.',\n",
        "                   'O Banco Nacional de Desenvolvimento Econômico e Social (#BNDES) adiou de 22 de agosto para o próximo dia 6 de setembro a realização de audiência pública para discutir a desestatização do Porto de Santos.']"
      ],
      "metadata": {
        "id": "onBJYU-ftUTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_tokens : List[List[str]] = tokenizar(teste)\n",
        "lista_tokens = trata_tokens_ausentes(lista_tokens, vocab)\n",
        "np_tokens: np.ndarray = converte_tokens_to_id(lista_tokens, vocab, MAX_SENTENCE_LENGTH )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzJMLqbesjCR",
        "outputId": "6c301fb9-3eab-4eb8-98ee-3dfe09fc09b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizando... : 100%|██████████| 5/5 [00:00<00:00, 247.32it/s]\n",
            "Tratamento tokens ausentes...: 100%|██████████| 5/5 [00:00<00:00, 4765.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_logits = model_load.predict(np_tokens)"
      ],
      "metadata": {
        "id": "SbFF2RQou4gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = np.argmax(predict_logits, axis=1)"
      ],
      "metadata": {
        "id": "gB1A3IxYvGRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, t in enumerate(teste):\n",
        "    print(f' Sentimento: {LABEL_NAMES[predict[index]]} /n {t}')\n",
        "    print(' ===============================================')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqTI9O-ZvLw8",
        "outputId": "5174a23b-fcb4-474a-d56f-b8c57b83dd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sentimento: Negativa /n Um amigo #Cubano está perguntando se o PT continuará investindo em #Cuba com dinheiro do #BNDES(do povo #brasileiro). Falei pra ele ficar tranquilo, pois #Lula fará trens, portos, aeroportos, e muito mais, em Cuba. Esse item do plano de governo do PT mostra bem isso, correto?\n",
            " ===============================================\n",
            " Sentimento: Positiva /n O Programa Emergencial de Acesso a Crédito, tem como objetivo possibilitar a ampliação do acesso ao crédito para MEIs, micro, pequenas e médias empresas, permitindo a manutenção do emprego e da renda.\n",
            " ===============================================\n",
            " Sentimento: Negativa /n Manda ele pegar o #Metro de Caracas pago pelo #BNDES  e levar pra  #BeloHorizonte. #PTNuncamais\n",
            " ===============================================\n",
            " Sentimento: Negativa /n Falei lá atrás que o #PTNuncaMais aparelhou junto com #Cuba, #Venezuela e mais 13 nações via #BNDES e me perdoem #patriotas não lembrar o nome da #Bolívia que foi sim ajudada pelo #bandido #comunista via #Petrobrás com 3 refinarias a custo zero para aquele país sul americano.\n",
            " ===============================================\n",
            " Sentimento: Neutra /n O Banco Nacional de Desenvolvimento Econômico e Social (#BNDES) adiou de 22 de agosto para o próximo dia 6 de setembro a realização de audiência pública para discutir a desestatização do Porto de Santos.\n",
            " ===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZPKtRPbOxFqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}