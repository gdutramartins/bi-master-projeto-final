{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "teste-bert-model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkVV8RHU8bNh",
        "outputId": "1763bfd1-d9c2-474b-adf5-ef829ccd50a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 28 10:14:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    33W / 250W |   3747MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Fw5l8KzgB_"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "from typing import List, Dict, Union, Tuple, NoReturn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "from transformers import BertTokenizerFast, BatchEncoding, PreTrainedTokenizerFast, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "from datasets import load_metric\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gc\n",
        "\n"
      ],
      "metadata": {
        "id": "L_TZyfNSzw4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QuiyhIYz2nR",
        "outputId": "26cf16f0-90fb-4ca2-f76b-39531011d926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXSDRnsA46Yq",
        "outputId": "16aab0d4-a8c7-45f1-d98e-0eed4da6190b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_PATH:str = '/content/drive/MyDrive'\n",
        "LABEL_NAMES: List[str] = ['Negativa', 'Neutra', 'Positiva']\n",
        "LABEL_DICT_CONV_MENCOES : Dict[int,str] = {-5:'Negativa', 0:'Neutra', 5:'Positiva'}\n",
        "\n",
        "MODEL_PATH: str = os.path.join(GDRIVE_PATH, 'model', 'projeto-final')\n",
        "MODEL_TRAINED_PATH: str = os.path.join(MODEL_PATH, 'bert')\n",
        "MODEL_TRAINED_LOG: str = os.path.join(MODEL_TRAINED_PATH, 'trainer.log')\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 512\n",
        "\n",
        "BASE_BERT_MODEL: str = 'neuralmind/bert-base-portuguese-cased'"
      ],
      "metadata": {
        "id": "33jnBcoA0ByU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_TRAINED_PATH, num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_BERT_MODEL, do_lower_case=False)\n",
        "model.to('cuda')"
      ],
      "metadata": {
        "id": "pfulz_TH0CIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teste: List[str] =[ 'Um amigo bndes #Cubano está perguntando se o PT continuará investindo em #Cuba com dinheiro do #BNDES(do povo #brasileiro). Falei pra ele ficar tranquilo, pois #Lula fará trens, portos, aeroportos, e muito mais, em Cuba. Esse item do plano de governo do PT mostra bem isso, correto?',\n",
        "                   'O Programa Emergencial de Acesso a Crédito, tem como objetivo possibilitar a ampliação do acesso ao crédito para MEIs, micro, pequenas e médias empresas, permitindo a manutenção do emprego e da renda.',\n",
        "                   'Manda ele pegar o #Metro de Caracas pago pelo #BNDES  e levar pra  #BeloHorizonte. #PTNuncamais',\n",
        "                   'Falei lá atrás que o #PTNuncaMais aparelhou junto com #Cuba, #Venezuela e mais 13 nações via #BNDES e me perdoem #patriotas não lembrar o nome da #Bolívia que foi sim ajudada pelo #bandido #comunista via #Petrobrás com 3 refinarias a custo zero para aquele país sul americano.',\n",
        "                   'O Banco Nacional de Desenvolvimento Econômico e Social (#BNDES) adiou de 22 de agosto para o próximo dia 6 de setembro a realização de audiência pública para discutir a desestatização do Porto de Santos.']"
      ],
      "metadata": {
        "id": "wXkYwvPq0OI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.batch_encode_plus(\n",
        "      teste,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "    )\n",
        "\n",
        "#      'texto': texto,\n",
        "#      'input_ids': encoding['input_ids'].flatten(),\n",
        "#      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "#      'labels': torch.tensor(sentimento, dtype=torch.long)\n",
        " "
      ],
      "metadata": {
        "id": "olEJwFPL0Ygc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72bed0c-1b67-4e75-9611-b657d309464b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding[0].tokens"
      ],
      "metadata": {
        "id": "A8yXWVlIBGY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.batch_encode_plus(\n",
        "      teste,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "    )\n",
        "\n",
        "model_predict = model(encoding['input_ids'].to('cuda'), encoding['attention_mask'].to('cuda'))\n",
        "model_predict_converted = F.softmax(model_predict.logits, dim=1).cpu().detach().numpy()\n",
        "predict = np.argmax(model_predict_converted, axis=1)\n",
        "for index, t in enumerate(teste):\n",
        "    print(f' Sentimento: {LABEL_NAMES[predict[index]]} \\n {t}')\n",
        "    print(' ===============================================')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mo_bvda25K2",
        "outputId": "9c15f31b-7dc5-427d-d1c1-e638135cad16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sentimento: Negativa \n",
            " Um amigo bndes #Cubano está perguntando se o PT continuará investindo em #Cuba com dinheiro do #BNDES(do povo #brasileiro). Falei pra ele ficar tranquilo, pois #Lula fará trens, portos, aeroportos, e muito mais, em Cuba. Esse item do plano de governo do PT mostra bem isso, correto?\n",
            " ===============================================\n",
            " Sentimento: Positiva \n",
            " O Programa Emergencial de Acesso a Crédito, tem como objetivo possibilitar a ampliação do acesso ao crédito para MEIs, micro, pequenas e médias empresas, permitindo a manutenção do emprego e da renda.\n",
            " ===============================================\n",
            " Sentimento: Neutra \n",
            " Manda ele pegar o #Metro de Caracas pago pelo #BNDES  e levar pra  #BeloHorizonte. #PTNuncamais\n",
            " ===============================================\n",
            " Sentimento: Negativa \n",
            " Falei lá atrás que o #PTNuncaMais aparelhou junto com #Cuba, #Venezuela e mais 13 nações via #BNDES e me perdoem #patriotas não lembrar o nome da #Bolívia que foi sim ajudada pelo #bandido #comunista via #Petrobrás com 3 refinarias a custo zero para aquele país sul americano.\n",
            " ===============================================\n",
            " Sentimento: Neutra \n",
            " O Banco Nacional de Desenvolvimento Econômico e Social (#BNDES) adiou de 22 de agosto para o próximo dia 6 de setembro a realização de audiência pública para discutir a desestatização do Porto de Santos.\n",
            " ===============================================\n"
          ]
        }
      ]
    }
  ]
}